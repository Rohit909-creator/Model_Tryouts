{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da853cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import pytorch_lightning as pl\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# with open(\"./dataset_cache/answer_to_idx.json\", 'r') as f:\n",
    "#     s = f.read()\n",
    "#     answer_to_idx = json.loads(s)\n",
    "\n",
    "# print(f\"Total unique answers: {len(answer_to_idx)}\")\n",
    "\n",
    "class CrossModalFusion(nn.Module):\n",
    "    \"\"\"Cross-modal fusion module to better combine image and text features\"\"\"\n",
    "    def __init__(self, llm_dim, image_dim):\n",
    "        super().__init__()\n",
    "        self.image_proj = nn.Linear(image_dim, llm_dim)\n",
    "        # self.text_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(llm_dim, num_heads=4, dropout=0.4, bias=False)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(llm_dim)\n",
    "        self.ln2 = nn.LayerNorm(llm_dim)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(llm_dim, llm_dim*2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(llm_dim*2, llm_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, hidden_state, image_embs):\n",
    "        # Project features\n",
    "        img_proj = self.image_proj(image_embs)\n",
    "        \n",
    "        out = self.cross_attn(hidden_state, img_proj, img_proj)[0]\n",
    "\n",
    "        out = self.ln1(out + hidden_state)\n",
    "        \n",
    "        out = self.ln2(self.mlp(out)+out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class AdapterTransformerLayer(nn.Module):\n",
    "    def __init__(self, transformer_layer, image_dim):\n",
    "        \"\"\"\n",
    "        A wrapper around an existing transformer layer that adds adapters after\n",
    "        attention and after the feed-forward sublayers.\n",
    "\n",
    "        Args:\n",
    "            transformer_layer: One layer from a pre-trained transformer (e.g., BERTLayer)\n",
    "            adapter_size: Bottleneck size for the adapters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = transformer_layer\n",
    "        self.hidden_size = transformer_layer.attn.c_attn.nx  # Model-specific\n",
    "\n",
    "        # Freeze all transformer weights (we donâ€™t train them)\n",
    "        for param in self.layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Add a CrossAttention Adapter\n",
    "        \n",
    "        self.adapter = CrossModalFusion(self.hidden_size, image_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, hidden_states, image_embeddings, encoder_attention_mask=None):\n",
    "        # Standard attention (output of frozen pre-trained layer)\n",
    "        \n",
    "        hidden_states = self.layer(hidden_states)\n",
    "\n",
    "        # Inject adapter after attention\n",
    "        fused_hidden_state = self.adapter(hidden_states, image_embeddings)\n",
    "        return fused_hidden_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165f6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added adapters to layers 6 to 11\n",
      "self.config.n_embed 768\n",
      "Total parameters: 170,898,438\n",
      "Trainable parameters: 46,457,094\n",
      "Percentage trainable: 27.18%\n",
      "Output logits shape: torch.Size([2, 20, 50259])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "class CrossModalFusion(nn.Module):\n",
    "    \"\"\"Improved cross-modal fusion module\"\"\"\n",
    "    def __init__(self, llm_dim, image_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.llm_dim = llm_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Project image features to match LLM dimension\n",
    "        self.image_proj = nn.Sequential(\n",
    "            nn.Linear(image_dim, llm_dim),\n",
    "            nn.LayerNorm(llm_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Cross-attention: text queries, image keys/values\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            llm_dim, \n",
    "            num_heads=num_heads, \n",
    "            dropout=dropout, \n",
    "            bias=False,\n",
    "            batch_first=True  # Important for proper batching\n",
    "        )\n",
    "        \n",
    "        # Layer norms\n",
    "        self.ln1 = nn.LayerNorm(llm_dim)\n",
    "        self.ln2 = nn.LayerNorm(llm_dim)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(llm_dim, llm_dim * 4),  # Standard 4x expansion\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(llm_dim * 4, llm_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Gating mechanism to control fusion strength\n",
    "        self.gate = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, text_hidden, image_embs, attention_mask=None):\n",
    "        batch_size, seq_len, hidden_dim = text_hidden[0].shape\n",
    "        \n",
    "        # Project image embeddings\n",
    "        img_proj = self.image_proj(image_embs)  # [B, num_patches, llm_dim]\n",
    "        # Cross-attention: text attends to image\n",
    "        attn_out, attn_weights = self.cross_attn(\n",
    "            query=text_hidden[0],\n",
    "            key=img_proj,\n",
    "            value=img_proj,\n",
    "            key_padding_mask=None,  # Could add image padding mask here\n",
    "            need_weights=True\n",
    "        )\n",
    "        # Apply gating and residual connection\n",
    "        text_hidden = self.ln1(text_hidden[0] + self.gate * attn_out)\n",
    "        # Feed-forward with residual\n",
    "        ff_out = self.mlp(text_hidden)\n",
    "        text_hidden = self.ln2(text_hidden + ff_out)\n",
    "        return text_hidden, attn_weights\n",
    "\n",
    "class AdapterTransformerLayer(nn.Module):\n",
    "    def __init__(self, transformer_layer, image_dim, adapter_position=\"after\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transformer_layer: Original GPT-2 transformer block\n",
    "            image_dim: Dimension of image embeddings\n",
    "            adapter_position: \"before\", \"after\", or \"parallel\" to the transformer block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = transformer_layer\n",
    "        self.adapter_position = adapter_position\n",
    "        \n",
    "        # Get hidden size from the transformer layer\n",
    "        self.hidden_size = transformer_layer.attn.c_attn.weight.shape[1] // 3  # Divide by 3 because it's concatenated Q,K,V\n",
    "        \n",
    "        # Freeze transformer weights\n",
    "        for param in self.layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add cross-modal adapter\n",
    "        self.adapter = CrossModalFusion(\n",
    "            llm_dim=self.hidden_size, \n",
    "            image_dim=image_dim,\n",
    "            num_heads=8,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "    def forward(self, hidden_states, image_embeddings=None, **kwargs):\n",
    "        if self.adapter_position == \"before\" and image_embeddings is not None:\n",
    "            # Apply adapter before transformer block\n",
    "            hidden_states, _ = self.adapter(hidden_states, image_embeddings)\n",
    "            hidden_states = self.layer(hidden_states, **kwargs)\n",
    "            \n",
    "        elif self.adapter_position == \"after\" and image_embeddings is not None:\n",
    "            # Apply transformer block first, then adapter\n",
    "            layer_outputs = self.layer(hidden_states, **kwargs)\n",
    "            # Extract hidden states from layer output (could be tuple or tensor)\n",
    "            if isinstance(layer_outputs, tuple):\n",
    "                hidden_states = layer_outputs[0]\n",
    "            else:\n",
    "                hidden_states = layer_outputs\n",
    "            \n",
    "            # Apply adapter\n",
    "            hidden_states, _ = self.adapter((hidden_states,), image_embeddings)\n",
    "            \n",
    "        elif self.adapter_position == \"parallel\" and image_embeddings is not None:\n",
    "            # Parallel processing with weighted combination\n",
    "            transformer_out = self.layer(hidden_states, **kwargs)\n",
    "            if isinstance(transformer_out, tuple):\n",
    "                transformer_out = transformer_out[0]\n",
    "            \n",
    "            adapter_out, _ = self.adapter((hidden_states,), image_embeddings)\n",
    "            \n",
    "            # Learnable combination weights\n",
    "            if not hasattr(self, 'combination_weight'):\n",
    "                self.combination_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            hidden_states = (1 - self.combination_weight) * transformer_out + \\\n",
    "                           self.combination_weight * adapter_out\n",
    "        else:\n",
    "            # No image embeddings, just use original transformer\n",
    "            layer_outputs = self.layer(hidden_states, **kwargs)\n",
    "            if isinstance(layer_outputs, tuple):\n",
    "                hidden_states = layer_outputs[0]\n",
    "            else:\n",
    "                hidden_states = layer_outputs\n",
    "            \n",
    "        return (hidden_states,)  # Return tuple to match GPT-2 output format\n",
    "\n",
    "class MultimodalGPT2(nn.Module):\n",
    "    \"\"\"Complete multimodal GPT-2 model\"\"\"\n",
    "    def __init__(self, gpt2_model_name=\"openai-community/gpt2\", image_dim=512, \n",
    "                 adapter_position=\"after\", num_adapter_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load base GPT-2 model\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "        \n",
    "        self.gpt2.requires_grad_(False)\n",
    "        # Add padding token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Store config\n",
    "        self.config = self.gpt2.config\n",
    "        self.image_dim = image_dim\n",
    "        \n",
    "        # Wrap selected layers with adapters\n",
    "        # Usually better to add adapters to later layers\n",
    "        num_layers = len(self.gpt2.transformer.h)\n",
    "        start_layer = max(0, num_layers - num_adapter_layers)\n",
    "        \n",
    "        for i in range(start_layer, num_layers):\n",
    "            original_layer = self.gpt2.transformer.h[i]\n",
    "            self.gpt2.transformer.h[i] = AdapterTransformerLayer(\n",
    "                original_layer, image_dim, adapter_position\n",
    "            )\n",
    "        \n",
    "        print(f\"Added adapters to layers {start_layer} to {num_layers-1}\")\n",
    "        \n",
    "        # Image feature processor (optional, if you want to process raw images)\n",
    "        print(\"self.config.n_embed\", self.config.n_embd)\n",
    "        self.image_processor = nn.Sequential(\n",
    "            nn.Linear(512, self.config.n_embd),\n",
    "            nn.LayerNorm(self.config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Special tokens for multimodal processing\n",
    "        self.image_start_token = \"<image>\"\n",
    "        self.image_end_token = \"</image>\"\n",
    "        \n",
    "    def add_special_tokens(self):\n",
    "        \"\"\"Add special tokens for image boundaries\"\"\"\n",
    "        special_tokens = {\n",
    "            \"additional_special_tokens\": [self.image_start_token, self.image_end_token]\n",
    "        }\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        self.gpt2.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, image_embeddings=None, \n",
    "                labels=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]  \n",
    "            image_embeddings: [batch_size, num_patches, image_dim]\n",
    "            labels: [batch_size, seq_len] for training\n",
    "        \"\"\"\n",
    "        # Process image embeddings if provided\n",
    "        if image_embeddings is not None:\n",
    "            image_embeddings = self.image_processor(image_embeddings)\n",
    "        \n",
    "        # Store image embeddings in a way that adapter layers can access them\n",
    "        # We'll modify the forward pass to pass this through\n",
    "        return self._forward_with_adapters(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            image_embeddings=image_embeddings,\n",
    "            labels=labels,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def _forward_with_adapters(self, input_ids, attention_mask=None, \n",
    "                              image_embeddings=None, labels=None, **kwargs):\n",
    "        \"\"\"Modified forward pass that handles image embeddings\"\"\"\n",
    "        \n",
    "        # Get token embeddings and position embeddings\n",
    "        inputs_embeds = self.gpt2.transformer.wte(input_ids)\n",
    "        position_ids = torch.arange(0, input_ids.size(-1), device=input_ids.device)\n",
    "        position_embeds = self.gpt2.transformer.wpe(position_ids)\n",
    "        \n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.gpt2.transformer.drop(hidden_states)\n",
    "        \n",
    "        # Process attention mask to the format expected by GPT-2\n",
    "        # GPT-2 expects a causal mask, we need to handle the attention mask properly\n",
    "        if attention_mask is not None:\n",
    "            # Convert attention mask to the format expected by GPT-2\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            # Create causal mask\n",
    "            causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=input_ids.device))\n",
    "            # Expand attention mask to match causal mask dimensions\n",
    "            expanded_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, S]\n",
    "            # Combine with causal mask\n",
    "            attention_mask = expanded_mask * causal_mask.unsqueeze(0).unsqueeze(0)  # [B, 1, S, S]\n",
    "            # Convert to the format expected by GPT-2 (large negative values for masked positions)\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)\n",
    "            attention_mask = (1.0 - attention_mask) * torch.finfo(hidden_states.dtype).min\n",
    "        \n",
    "        # Pass through transformer blocks (some have adapters)\n",
    "        for i, block in enumerate(self.gpt2.transformer.h):\n",
    "            if isinstance(block, AdapterTransformerLayer):\n",
    "                # Pass image embeddings to adapter layers\n",
    "                layer_outputs = block(hidden_states, \n",
    "                                    image_embeddings=image_embeddings, \n",
    "                                    attention_mask=attention_mask)\n",
    "                hidden_states = layer_outputs[0]\n",
    "                \n",
    "            else:\n",
    "                # Regular transformer block\n",
    "                layer_outputs = block(hidden_states, attention_mask=attention_mask)\n",
    "                if isinstance(layer_outputs, tuple):\n",
    "                    hidden_states = layer_outputs[0]\n",
    "                else:\n",
    "                    hidden_states = layer_outputs\n",
    "\n",
    "        # Final layer norm\n",
    "        hidden_states = self.gpt2.transformer.ln_f(hidden_states)\n",
    "        # Language modeling head\n",
    "        logits = self.gpt2.lm_head(hidden_states)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                           shift_labels.view(-1))\n",
    "            \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'loss': loss,\n",
    "            'hidden_states': hidden_states\n",
    "        }\n",
    "    \n",
    "    def generate_with_images(self, text, image_embeddings, max_length=100, **kwargs):\n",
    "        \"\"\"Generate text conditioned on both text and images\"\"\"\n",
    "        # Tokenize input text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Generate with image conditioning\n",
    "        with torch.no_grad():\n",
    "            outputs = self.gpt2.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs.get('attention_mask'),\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                **kwargs\n",
    "            )\n",
    "            outputs = self.gpt2.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs.get('attention_mask'),\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage and training setup\n",
    "def create_model_and_count_params():\n",
    "    # Create model\n",
    "    model = MultimodalGPT2(\n",
    "        gpt2_model_name=\"openai-community/gpt2\",\n",
    "        image_dim=768,  # Vision Transformer patch embeddings\n",
    "        adapter_position=\"after\",\n",
    "        num_adapter_layers=6\n",
    "    )\n",
    "    \n",
    "    # Add special tokens\n",
    "    model.add_special_tokens()\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example of how to prepare data\n",
    "def prepare_multimodal_batch(texts, image_features, tokenizer, max_length=768):\n",
    "    \"\"\"\n",
    "    Prepare a batch of multimodal data\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        image_features: Tensor of shape [batch_size, num_patches, feature_dim]\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        max_length: Maximum sequence length\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask'],\n",
    "        'image_embeddings': image_features,\n",
    "        'labels': encodings['input_ids'].clone()  # For language modeling\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = create_model_and_count_params()\n",
    "    \n",
    "    # Example forward pass\n",
    "    batch_size, seq_len = 2, 20\n",
    "    num_patches, image_dim = 196, 512  # Typical ViT-Base patch embeddings\n",
    "    \n",
    "    # Dummy data\n",
    "    input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "    image_embeddings = torch.randn(batch_size, num_patches, image_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, image_embeddings=image_embeddings)\n",
    "    print(f\"Output logits shape: {outputs['logits'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a49c0432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(r\"C:\\Users\\Rohit Francis\\Downloads\\FinalModel.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fac0725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.9883   0.00925  0.002338]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model2, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open('Clip.jpg')).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a clip\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model2.encode_image(image)\n",
    "    text_features = model2.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model2(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "189022c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03a405dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"what is in this image?\"\n",
    "inputs = model.tokenizer(text, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc2938f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**inputs, image_features = image_features.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9aea06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 50259])\n",
      "tensor([[  11,  262,  262, 1492,   30,  198]])\n"
     ]
    }
   ],
   "source": [
    "print(out['logits'].shape)\n",
    "pred_tokens = torch.argmax(out[\"logits\"], dim=-1)\n",
    "print(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a97b8320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', the the book?\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(pred_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "421fce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", you think in?\"\n",
      "\n",
      ", you think in?\" I're? the\n",
      "\n",
      "\n",
      ", you think in?\" I're? the\n",
      "\n",
      " you think in?\" \"'m not\n",
      " one\n",
      "\n",
      ", you think in?\" I're? the\n",
      "\n",
      " you think in?\" \"'m not\n",
      " one\n",
      "\" you're in? I'm? the I\n",
      "'re in?\n",
      "Cause',\n",
      " of\n",
      "\n",
      ", you think in?\" I're? the\n",
      "\n",
      " you think in?\" \"'m not\n",
      " one\n",
      "\" you're in? I'm? the I\n",
      "'re in?\n",
      "Cause',\n",
      " of\n",
      " you think in?\" \"'m? the\n",
      "\n",
      " think in?\" \"'m not one\n",
      "\n",
      ",I think not?\" the'm not the\n",
      "\n",
      "'m\n",
      "??\" the\n",
      " I you\n",
      " course\"\n",
      ", you think in?\" I're? the\n",
      "\n",
      " you think in?\" \"'m not\n",
      " one\n",
      "\" you're in? I'm? the I\n",
      "'re in?\n",
      "Cause',\n",
      " of\n",
      " you think in?\" \"'m? the\n",
      "\n",
      " think in?\" \"'m not one\n",
      "\n",
      ",I think not?\" the'm not the\n",
      "\n",
      "'m\n",
      "??\" the\n",
      " I you\n",
      " course\" of think in?\"\n",
      "'m? the\n",
      "\n",
      "\n",
      " think in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in? I'm? the I'm\n",
      " in?\n",
      "\n",
      "', of\n",
      "\n",
      "\" think in?\" Im not\n",
      "\n",
      "\n",
      " in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you'm in\n",
      " I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" not\n",
      " the the\n",
      "\n",
      "\n",
      "'m think\n",
      "\n",
      "\n",
      "\n",
      ", you think in?\" I're? the\n",
      "\n",
      " you think in?\" \"'m not\n",
      " one\n",
      "\" you're in? I'm? the I\n",
      "'re in?\n",
      "Cause',\n",
      " of\n",
      " you think in?\" \"'m? the\n",
      "\n",
      " think in?\" \"'m not one\n",
      "\n",
      ",I think not?\" the'm not the\n",
      "\n",
      "'m\n",
      "??\" the\n",
      " I you\n",
      " course\" of think in?\"\n",
      "'m? the\n",
      "\n",
      "\n",
      " think in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in? I'm? the I'm\n",
      " in?\n",
      "\n",
      "', of\n",
      "\n",
      "\" think in?\" Im not\n",
      "\n",
      "\n",
      " in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you'm in\n",
      " I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" not\n",
      " the the\n",
      "\n",
      "\n",
      "'m think\n",
      "\n",
      "\n",
      " think in?\"\n",
      "'m? the\n",
      "\n",
      "\n",
      " think in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in? I'm? the I\n",
      "\n",
      " in?\n",
      "\n",
      "', of\n",
      " you, think in?\" \"'m not the\n",
      "\n",
      "\n",
      " in?\" \"'m not one\n",
      "\n",
      "\" you think not?\" the'm not the\n",
      "\n",
      "'\n",
      "\n",
      "?\" the I\n",
      " you\n",
      "\n",
      "\", you\n",
      " in?\" I\n",
      " not the\n",
      "\n",
      "\n",
      "you in?\" \"'m not one\n",
      "\"I're in?\" I'm? the I\n",
      "\n",
      "\n",
      "?\n",
      "\n",
      "Cause of you\n",
      ", you in?\" I not\n",
      "\n",
      "\"?\" \"'m not\n",
      "\n",
      "\" you're in?\n",
      "'m\n",
      "\n",
      "\"\" you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" not in\n",
      "\"\n",
      ", you think in?\" I're? the\n",
      "\n",
      " you think in?\" \"'m not\n",
      " one\n",
      "\" you're in? I'm? the I\n",
      "'re in?\n",
      "Cause',\n",
      " of\n",
      " you think in?\" \"'m? the\n",
      "\n",
      " think in?\" \"'m not one\n",
      "\n",
      ",I think not?\" the'm not the\n",
      "\n",
      "'m\n",
      "??\" the\n",
      " I you\n",
      " course\" of think in?\"\n",
      "'m? the\n",
      "\n",
      "\n",
      " think in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in? I'm? the I'm\n",
      " in?\n",
      "\n",
      "', of\n",
      "\n",
      "\" think in?\" Im not\n",
      "\n",
      "\n",
      " in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you'm in\n",
      " I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" not\n",
      " the the\n",
      "\n",
      "\n",
      "'m think\n",
      "\n",
      "\n",
      " think in?\"\n",
      "'m? the\n",
      "\n",
      "\n",
      " think in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in? I'm? the I\n",
      "\n",
      " in?\n",
      "\n",
      "', of\n",
      " you, think in?\" \"'m not the\n",
      "\n",
      "\n",
      " in?\" \"'m not one\n",
      "\n",
      "\" you think not?\" the'm not the\n",
      "\n",
      "'\n",
      "\n",
      "?\" the I\n",
      " you\n",
      "\n",
      "\", you\n",
      " in?\" I\n",
      " not the\n",
      "\n",
      "\n",
      "you in?\" \"'m not one\n",
      "\"I're in?\" I'm? the I\n",
      "\n",
      "\n",
      "?\n",
      "\n",
      "Cause of you\n",
      ", you in?\" I not\n",
      "\n",
      "\"?\" \"'m not\n",
      "\n",
      "\" you're in?\n",
      "'m\n",
      "\n",
      "\"\" you\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" not in\n",
      "\"\n",
      " think in?\" I'm? the\n",
      "\n",
      "\n",
      " think in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in? I'm? the I'm\n",
      " in?\n",
      "\n",
      "', of\n",
      "\n",
      "\" think in?\" \"'m not the\n",
      "\n",
      "\n",
      " in?\" \"'m not one\n",
      "\n",
      "\"I think not?\" the'm not the\n",
      "\n",
      "'\n",
      "\n",
      "?\" the I\n",
      " you\n",
      "\n",
      "\" of think in?\"\n",
      "\n",
      "? the\n",
      "\n",
      "\n",
      "\" in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in\n",
      " I'm? the I'm\n",
      "\n",
      "?\n",
      "\n",
      "', of\n",
      "\n",
      "\" think in?\" Im not\n",
      "\n",
      "\n",
      " in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\" not the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'m think\n",
      "\n",
      "\n",
      ", in?\" I\n",
      "? the\n",
      "\n",
      "\n",
      "\" in?\" \"'m not\n",
      "\n",
      "\n",
      "\" you're in\n",
      " I'm? the I'm\n",
      "?\"\n",
      "\n",
      "', of\n",
      "\n",
      " think you in?\" Imm not\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"?\" \"'m not\n",
      "\n",
      "\n",
      "\n",
      "\" you're in in I\n",
      "\n",
      " not the\n",
      "\n",
      "\n",
      "'m\n",
      "\" the I you\n",
      "\n",
      "\n",
      "course you think\n",
      "?\" Im'm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" think?\" \"'m not\n",
      "\n",
      "\n",
      "\n",
      " you'm not? Im'm? the\n",
      "\n",
      "\n",
      "\n",
      "in\n",
      "\n",
      "\",\n",
      "\n",
      "\n",
      " you think?\" I'm the\n",
      "\" you them not one\n",
      "\" you're in?\" I\n",
      " not\n",
      "?\"\n",
      "'re\n",
      "\n",
      "\" not\n",
      "?\"\n",
      " you\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred_text\n\u001b[0;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, image_features \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(out['logits'].shape)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m pred_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 203\u001b[0m, in \u001b[0;36mMultimodalGPT2.forward\u001b[1;34m(self, input_ids, attention_mask, image_embeddings, labels, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(image_embeddings)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Store image embeddings in a way that adapter layers can access them\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# We'll modify the forward pass to pass this through\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_with_adapters(\n\u001b[0;32m    204\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    205\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    206\u001b[0m     image_embeddings\u001b[38;5;241m=\u001b[39mimage_embeddings,\n\u001b[0;32m    207\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    209\u001b[0m )\n",
      "Cell \u001b[1;32mIn[15], line 218\u001b[0m, in \u001b[0;36mMultimodalGPT2._forward_with_adapters\u001b[1;34m(self, input_ids, attention_mask, image_embeddings, labels, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[0;32m    217\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 218\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[0;32m    221\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "text = \"what do you see here?\"\n",
    "pred_text = \"\"\n",
    "for i in range(10):\n",
    "    text += pred_text\n",
    "    inputs = model.tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    out = model(**inputs, image_features = image_features.detach())\n",
    "    # print(out['logits'].shape)\n",
    "    pred_tokens = torch.argmax(out[\"logits\"], dim=-1)\n",
    "    # print(pred_tokens)\n",
    "    pred_text = model.tokenizer.decode(pred_tokens[0], skip_special_tokens=True)\n",
    "    print(pred_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7f58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28869c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be33dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPT2 Blocks 12\n"
     ]
    }
   ],
   "source": [
    "# Define adapter size\n",
    "adapter_size = 768\n",
    "num_blocks = len(model.transformer.h)\n",
    "print(f\"Num GPT2 Blocks\", num_blocks)\n",
    "\n",
    "# Wrap all encoder layers with adapter-enabled versions\n",
    "for i in range(num_blocks):\n",
    "    original_layer = model.transformer.h[i]\n",
    "    # print(original_layer)\n",
    "    \n",
    "    model.transformer.h[i] = AdapterTransformerLayer(original_layer, adapter_size)\n",
    "# Check that only adapters will be trained\n",
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Trainable parameters: {trainable_params} / {total_params}\")\n",
    "\n",
    "# # Now you can tokenize input and train like usual.\n",
    "# inputs = tokenizer(\"Adapters are lightweight and powerful.\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6896912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x AdapterTransformerLayer(\n",
       "    (layer): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (adapter): CrossModalFusion(\n",
       "      (image_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (cross_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "216b68ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AdapterTransformerLayer.forward() got multiple values for argument 'encoder_attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHey yesterday I had a croissant from Walmart.\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:927\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    925\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m--> 927\u001b[0m outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[0;32m    928\u001b[0m     hidden_states,\n\u001b[0;32m    929\u001b[0m     past_key_values \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    930\u001b[0m     cache_position,\n\u001b[0;32m    931\u001b[0m     causal_mask,\n\u001b[0;32m    932\u001b[0m     head_mask[i],\n\u001b[0;32m    933\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    934\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    935\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    936\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    938\u001b[0m )\n\u001b[0;32m    940\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Rohit Francis\\Desktop\\Codes\\AI_Projects\\Model_Tryouts\\deeplearningenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: AdapterTransformerLayer.forward() got multiple values for argument 'encoder_attention_mask'"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hey yesterday I had a croissant from Walmart.\", return_tensors='pt')\n",
    "\n",
    "output = model(**inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    # Load data\n",
    "    X1 = torch.load(\"./dataset_cache/images_embs.pt\", weights_only=True)\n",
    "    X2 = torch.load(\"./dataset_cache/text_embs.pt\", weights_only=True)\n",
    "    print('Shape of data:', X1.shape, X2.shape, X1.dtype, X2.dtype)\n",
    "    \n",
    "    # Convert to float32 if needed\n",
    "    X1 = X1.to(torch.float32)\n",
    "    X2 = X2.to(torch.float32)\n",
    "    \n",
    "    # Normalize embeddings (important for consistent scale)\n",
    "    X1 = F.normalize(X1, p=2, dim=1)\n",
    "    X2 = F.normalize(X2, p=2, dim=1)\n",
    "    \n",
    "    # Load targets\n",
    "    Y = torch.load(\"./dataset_cache/targets.pt\", weights_only=True)\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    eye = torch.eye(len(answer_to_idx))[Y.squeeze()]\n",
    "    \n",
    "    # Print some stats\n",
    "    print(f\"Total samples: {len(Y)}\")\n",
    "    print(f\"Feature dimensions: Image={X1.shape[1]}, Text={X2.shape[1]}\")\n",
    "    print(f\"Target classes: {len(answer_to_idx)}\")\n",
    "    \n",
    "    return TensorDataset(X1, X2, eye)\n",
    "\n",
    "def get_dataloaders(batch_size=32):\n",
    "    dataset = create_data()\n",
    "    \n",
    "    # Create train/val/test split (80/10/10)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    # val_size = int(0.2 * len(dataset))\n",
    "    val_size = len(dataset)-train_size\n",
    "    # test_size = len(dataset) - train_size - val_size\n",
    "    \n",
    "    # Create splits with fixed seed for reproducibility\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # test_loader = DataLoader(\n",
    "    #     test_dataset, \n",
    "    #     batch_size=batch_size, \n",
    "    #     shuffle=False,\n",
    "    #     num_workers=4,\n",
    "    #     pin_memory=True\n",
    "    # )\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    # print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Train model\n",
    "def train_model(max_epochs=30, batch_size=64):\n",
    "    # Create model and dataloaders\n",
    "    model = EnhancedFusor(\n",
    "        embedding_size=512, \n",
    "        num_heads=8,\n",
    "        dropout=0.2,\n",
    "        lr=5e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    train_loader, val_loader = get_dataloaders(batch_size)\n",
    "\n",
    "    # Create trainer with additional callbacks\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        enable_progress_bar=True,\n",
    "        num_nodes=1,\n",
    "        enable_checkpointing=True,\n",
    "        callbacks=[\n",
    "            pl.callbacks.ModelCheckpoint(\n",
    "                monitor='val_loss',\n",
    "                filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}',\n",
    "                save_top_k=3,\n",
    "                mode='min'\n",
    "            ),\n",
    "            # pl.callbacks.EarlyStopping(\n",
    "            #     monitor='val_loss',\n",
    "            #     patience=5,\n",
    "            #     mode='min'\n",
    "            # ),\n",
    "        ],\n",
    "        gradient_clip_val=1.0,  # Prevent exploding gradients\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Test the model\n",
    "    # test_result = trainer.test(model, test_loader)\n",
    "    # print(f\"Test results: {test_result}\")\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "def inference_example(checkpoint_path=None):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create reverse mapping from index to answer\n",
    "    idx2answers = {v: k for k, v in answer_to_idx.items()}\n",
    "    \n",
    "    # Load model from checkpoint or create new\n",
    "    if checkpoint_path:\n",
    "        model = EnhancedFusor.load_from_checkpoint(\n",
    "            checkpoint_path=checkpoint_path\n",
    "        ).to(device)\n",
    "        print(f\"Loaded model from {checkpoint_path}\")\n",
    "    else:\n",
    "        model = EnhancedFusor().to(device)\n",
    "        print(\"Created new model (not trained)\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    _, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    image_path = r\"C:\\Users\\Rohit Francis\\Desktop\\Codes\\Datasets\\VQA\\dataset\\images\\image1.png\"  # Adjust path as needed\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Create text query\n",
    "    # query = \"what is on the left side of white oven ?\"\n",
    "    query = \"how many garbage_bin is here in the image ?\"\n",
    "    text = clip.tokenize([query]).to(device)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        image_embs = model.encode_image(image).to(torch.float32)\n",
    "        text_embs = model.encode_text(text).to(torch.float32)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        out = model(image_embs, text_embs)\n",
    "        \n",
    "    # Get top-5 predictions\n",
    "    top_probs, top_indices = torch.topk(F.softmax(out, dim=-1), k=5)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nTop 5 predictions:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0])):\n",
    "        answer = idx2answers.get(idx.item(), \"unknown\")\n",
    "        print(f\"{i+1}. {answer} ({prob.item()*100:.2f}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment to train the model\n",
    "    # train_model(max_epochs=30, batch_size=64)\n",
    "    inference_example(\"./lightning_logs/version_4/checkpoints/epoch=7-val_loss=3.66-val_acc=0.32.ckpt\")\n",
    "    # For inference with trained model (replace with your checkpoint path)\n",
    "    # inference_example(checkpoint_path=\"./lightning_logs/version_0/checkpoints/epoch=1-step=686.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
