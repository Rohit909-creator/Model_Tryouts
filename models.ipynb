{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d96a37a",
   "metadata": {},
   "source": [
    "# GPT-OSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1c7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 128])\n",
      "torch.Size([1, 100, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtune.modules as modules\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtune.modules as modules\n",
    "\n",
    "class GPT_OSS_Block(nn.Module):\n",
    "    def __init__(self, d_dim, max_seq_len, num_heads=8, num_kv_heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = d_dim // num_heads\n",
    "        \n",
    "        # Rotary embeddings applied per head\n",
    "        self.rope = modules.RotaryPositionalEmbeddings(\n",
    "            dim=self.head_dim, max_seq_len=max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Define the projection layers\n",
    "        q_proj = nn.Linear(d_dim, d_dim, bias=False)\n",
    "        k_proj = nn.Linear(d_dim, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        v_proj = nn.Linear(d_dim, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        out_proj = nn.Linear(d_dim, d_dim, bias=False)\n",
    "        \n",
    "        # Torchtune MultiHeadAttention requires you to provide them\n",
    "        self.gqa = modules.attention.MultiHeadAttention(\n",
    "            embed_dim=d_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_kv_heads=num_kv_heads,\n",
    "            head_dim=self.head_dim,\n",
    "            q_proj=q_proj,\n",
    "            k_proj=k_proj,\n",
    "            v_proj=v_proj,\n",
    "            output_proj=out_proj,\n",
    "        )\n",
    "        \n",
    "        self.rmsnorm1 = modules.RMSNorm(d_dim)\n",
    "        self.rmsnorm2 = modules.RMSNorm(d_dim)\n",
    "        \n",
    "        self.moe = MoE(d_dim, d_ff=d_dim*2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        embs = self.rmsnorm1(x)\n",
    "        \n",
    "        # --- Project to Q, K, V ---\n",
    "        q = self.gqa.q_proj(embs).view(B, L, self.num_heads, self.head_dim)\n",
    "        k = self.gqa.k_proj(embs).view(B, L, self.num_kv_heads, self.head_dim)\n",
    "        v = self.gqa.v_proj(embs).view(B, L, self.num_kv_heads, self.head_dim)\n",
    "        # print(q.shape)\n",
    "        # --- Apply RoPE ---\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "        # print(q.shape)\n",
    "        # --- Run scaled dot product attention (torchtune already has helper) ---\n",
    "        attn_out = self.gqa._attention_call(q, k, v,mask=None,        # no mask here\n",
    "            dropout_p=0.1,    # inference\n",
    "            is_causal=True)\n",
    "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n",
    "        attn_out = self.gqa.output_proj(attn_out)\n",
    "        # Residual + MoE\n",
    "        # print(attn_out.shape)\n",
    "        embs = attn_out + embs\n",
    "        embs = embs.view(B, L, -1)\n",
    "        \n",
    "        embs = self.rmsnorm2(embs)\n",
    "        embs = embs + self.moe(embs)\n",
    "        \n",
    "        return embs\n",
    "\n",
    "\n",
    "        \n",
    "class MoE(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_dim, d_ff, num_experts=32, top_k = 4, capacity_factor = 1.25, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.router = nn.Linear(d_dim, num_experts)\n",
    "        \n",
    "        self.experts = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(d_dim, d_ff),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_ff, d_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) for _ in range(self.num_experts)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        bsz, seq_len, d_model = x.shape\n",
    "        \n",
    "        logits = self.router(x)\n",
    "        \n",
    "        gate_probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        topk_vals, topk_idx = torch.topk(gate_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        topk_vals = topk_vals/topk_vals.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        out = torch.zeros_like(x)\n",
    "        for k in range(self.top_k):\n",
    "            idx =  topk_idx[..., k]\n",
    "            prob = topk_vals[..., k].unsqueeze(-1)\n",
    "            \n",
    "            for expert_id in range(self.num_experts):\n",
    "                mask = (idx==expert_id)\n",
    "                \n",
    "                if mask.any():\n",
    "                    expert_in = x[mask]\n",
    "                    expert_out = self.experts[expert_id](expert_in)\n",
    "                    out[mask] += prob[mask]*expert_out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "moe = MoE(128, 256)\n",
    "x = torch.randn((1, 100, 128))\n",
    "out = moe(x)\n",
    "print(out.shape)\n",
    "\n",
    "# input_ids = torch.randint(0, 100, (1, 100))\n",
    "model = GPT_OSS_Block(128, 100)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390e202",
   "metadata": {},
   "source": [
    "# Reasoning + Clarification Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load the trained model from the output directory\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"outputs\",  # Your output directory\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Save as GGUF for Ollama\n",
    "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "print(\"Model saved as GGUF format in 'model' directory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d34204",
   "metadata": {},
   "source": [
    "# Qformer Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db14e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from qformer import QFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f01266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, 32, 512))\n",
    "\n",
    "img = torch.randn((1,3,224,224))\n",
    "\n",
    "# Create an instance of the QFormer model with the following parameters:\n",
    "# - input_size: 512\n",
    "# - num_heads: 8\n",
    "# - num_layers: 8\n",
    "# - dropout: 0.1\n",
    "# - num_classes: 2\n",
    "# - num_patches: 2\n",
    "q_former = QFormer(512, 8, 8, 0.1, 2, 2)\n",
    "y = q_former(x, img)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ad4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Train_data.jsonl\", 'r') as f:\n",
    "    # while f.next():\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fafa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b482e53",
   "metadata": {},
   "source": [
    "# Surprise Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "663d81b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[11.3411,  6.9629]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SurpriseMechanism(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_slots, slot_dim, learning_rate = 0.6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.slots = torch.randn((1, num_slots, slot_dim))\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, slot_dim)\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        proj = self.fc1(x)\n",
    "    \n",
    "        \n",
    "        surprise = nn.functional.mse_loss(self.slots, proj.expand_as(self.slots), reduction='none').sum(dim=-1)\n",
    "        \n",
    "        min_index = torch.argmin(surprise, dim=-1)\n",
    "        print(min_index)\n",
    "        \n",
    "        self.slots.data[0][min_index] = (1-self.lr)*self.slots.data[0][min_index] + self.lr*proj\n",
    "        \n",
    "        return surprise        \n",
    "        \n",
    "\n",
    "\n",
    "model = SurpriseMechanism(4,2,8)\n",
    "\n",
    "x_t = torch.randn((1, 4))\n",
    "\n",
    "out = model(x_t)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ac1909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.slots.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b578dd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2288,  0.0145,  0.2790,  1.2779],\n",
       "         [ 1.9375, -0.7367, -2.1079, -0.0094]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slots = torch.randn((1, 2, 4))\n",
    "slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9001fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.expand_as(slots).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc3bf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1775,  1.4299,  0.2057, -0.6213]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3d0d15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1775,  1.4299,  0.2057, -0.6213],\n",
       "         [-0.1775,  1.4299,  0.2057, -0.6213]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.expand_as(slots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8901d2e",
   "metadata": {},
   "source": [
    "# HRM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd289c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 0\n",
      "at 4\n",
      "at 8\n",
      "at 12\n",
      "at 16\n",
      "at 20\n",
      "at 24\n",
      "at 28\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HRM(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size, context_length, output_size, h_cycle = 4, l_cycle = 8, device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h_cycle = h_cycle\n",
    "        self.l_cycle = l_cycle\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_embed = nn.Embedding(context_length, hidden_size)\n",
    "        self.low = nn.GRUCell(input_size=hidden_size*4, hidden_size=hidden_size*4, device=device,)\n",
    "        self.high = nn.GRUCell(input_size=hidden_size*4, hidden_size=hidden_size*4, device=device)\n",
    "        \n",
    "        # self.low = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size, device=device,)\n",
    "        # self.high = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size, device=device)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size*4, hidden_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size*2, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        token_embs = self.token_embed(tokens)\n",
    "        pos_embs = self.pos_embed(torch.arange(0, tokens.shape[-1]).to(tokens.device))\n",
    "        \n",
    "        embs = token_embs+pos_embs\n",
    "        embs = embs.view(tokens.shape[0], -1)\n",
    "        # print(embs.shape)\n",
    "        # hx, cx = torch.zeros((tokens.shape[0],pos_embs.shape[-1])), torch.zeros((tokens.shape[0],pos_embs.shape[-1]))\n",
    "        z_l = torch.zeros((tokens.shape[0],embs.shape[-1]))\n",
    "        # print(z_l.shape)\n",
    "        for i in range(self.h_cycle*self.l_cycle):\n",
    "            z_l = self.low(embs, z_l)\n",
    "            if i%self.h_cycle == 0: \n",
    "                # print(f\"at {i}\")\n",
    "                z_h = self.high(embs, z_l)\n",
    "                z_l = z_h\n",
    "        # print('here')\n",
    "        out = self.mlp(z_h)\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = HRM(32, 8, 4, 8)\n",
    "\n",
    "x = torch.randint(0,8, (1,4))\n",
    "out = model(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feabb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=1, embed_dim=512, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, patch_size, patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.proj(x)\n",
    "        out = out.flatten(2)\n",
    "        return out\n",
    "\n",
    "class HRMVision(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size,in_channels=4, sequence_length = 16, patch_size=64, embed_dim=16, h_cycle = 4, l_cycle = 8, device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.h_cycle = h_cycle\n",
    "        self.l_cycle = l_cycle\n",
    "        self.context_length =  16\n",
    "        self.patchify = PatchEmbedding(in_channels, sequence_length, patch_size)\n",
    "        \n",
    "        # self.token_embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_embed = nn.Embedding(self.context_length, embed_dim)\n",
    "        self.low = nn.GRUCell(input_size=embed_dim*embed_dim, hidden_size=embed_dim*embed_dim, device=device,)\n",
    "        self.high = nn.GRUCell(input_size=embed_dim*embed_dim, hidden_size=embed_dim*embed_dim, device=device)\n",
    "        \n",
    "        # self.low = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size, device=device,)\n",
    "        # self.high = nn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size, device=device)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim*embed_dim, embed_dim*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim*embed_dim, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        token_embs = self.patchify(image)\n",
    "        \n",
    "        pos_embs = self.pos_embed(torch.arange(0, self.context_length).to(image.device))\n",
    "        embs = token_embs+pos_embs\n",
    "        embs = embs.view(image.shape[0], -1)\n",
    "        # hx, cx = torch.zeros((tokens.shape[0],pos_embs.shape[-1])), torch.zeros((tokens.shape[0],pos_embs.shape[-1]))\n",
    "        z_l = torch.zeros((image.shape[0],embs.shape[-1]))\n",
    "        # print(z_l.shape)\n",
    "        for i in range(self.h_cycle*self.l_cycle):\n",
    "            z_l = self.low(embs, z_l)\n",
    "            if i%self.h_cycle == 0: \n",
    "                # print(f\"at {i}\")\n",
    "                z_h = self.high(embs, z_l)\n",
    "                z_l = z_h\n",
    "        # print('here')\n",
    "        out = self.mlp(z_h)\n",
    "        return out\n",
    "\n",
    "model = HRMVision(output_size=10, in_channels=1)\n",
    "\n",
    "x = torch.randn((1,1, 256, 256))\n",
    "out = model(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfdb5559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of HRMVision(\n",
       "  (patchify): PatchEmbedding(\n",
       "    (proj): Conv2d(1, 16, kernel_size=(64, 64), stride=(64, 64))\n",
       "  )\n",
       "  (pos_embed): Embedding(16, 16)\n",
       "  (low): GRUCell(256, 256)\n",
       "  (high): GRUCell(256, 256)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a48032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
